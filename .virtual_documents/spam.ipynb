import pandas as pd
from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer
import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet,stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
import string
from spellchecker import SpellChecker
from collections import Counter
from nltk.util import ngrams
from symspellpy import SymSpell, Verbosity
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import numpy as np
import matplotlib.pyplot as plt


df = pd.read_csv("Spam_SMS.csv")


df.shape


class Transformer():
    def __doc__(self):
        """
            Transformer Class to clean and prepare SMS Ham data:

            Attributes:
            -----------
                self.spam_bank (list):

                self.ham_bank  (list):

                self.spam_urls (list):

                self.ham_urls  (list):

                self.features  (pd.DataFrame):

                self.df  (pd.DataFrame):

                self.stopwords (list):

                self.lemmatizer (nltk.stem.WordNetLemmatizer):

                self.sia (nltk.sentiment.vader.SentimentIntensityAnalyzer):

                self.contractions (dict):

            Methods:
            --------
                self.tokenize_words(self, message: str) -> list:
        """

    def __init__(self, df: pd.DataFrame):
        self.spam_bank = [l for l in df[df["Class"] == "spam"]['Message']]
        # self.spam = ' '.join(df[df['Class'] == 'spam']["Message"])
        self.ham_bank = [l for l in df[df["Class"] == "ham"]["Message"]]
        self.spam_urls = [url for msg in self.spam_bank for url in re.findall(r'http[s]:\/\/[\S]+', msg)]
        self.ham_urls = [url for msg in self.ham_bank for url in re.findall(r'http[s]:\/\/[\S]+', msg)]
        self.features = pd.DataFrame({})
        self.stopwords = set(stopwords.words('english'))
        self.df = df.copy()
        self.lemmatizer = WordNetLemmatizer()
        self.sia = SentimentIntensityAnalyzer()
        self.contractions = {
                                "can't": "cannot",
                                "won't": "will not",
                                "n't": " not",
                                "'re": " are",
                                "'s": " is",
                                "'d": " would",
                                "'ll": " will",
                                "'ve": " have",
                                "'m": " am",
                            }

    def tokenize_words(self, message: str) -> list:
        tokenizer = RegexpTokenizer(r"[^\s.,?!]+")
        tokens = tokenizer.tokenize(message)
        tokens = [t.lower() for t in tokens]
        
        return tokens
    
    def expand_contractions(self, text, contractions_dict):
        contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())), 
                                        flags=re.IGNORECASE | re.DOTALL)
        def replace(match):
            return contractions_dict[match.group(0).lower()]
        
        return contractions_pattern.sub(replace, text)
    
    def clean_msg(self):
        self.features['clean_msg'] = self.df['Message'].str.lower()
        self.features["target"] = self.df["Class"].apply(lambda x: 1 if x == "spam" else 0)
        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: self.expand_contractions(x, self.contractions))
        self.features['clean_msg'] = self.features['clean_msg'].str.replace(r'http[s]:\/\/[\S]+', '<url>', regex=True)
        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: x.translate(str.maketrans('','', string.punctuation)))
        
        sym_spell = SymSpell(max_dictionary_edit_distance=4, prefix_length=7)
        dictionary = "frequency_dictionary_en_82_765.txt"
        sym_spell.load_dictionary(dictionary,term_index=0,count_index=1)

        def correct(msg: str) -> str:
            suggestions = []
            for word in msg.split():
                suggestion = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)
                if suggestion:
                    suggestions.append(suggestion[0].term)
                else:
                    suggestions.append(word)

            return " ".join(suggestions)
        
        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: correct(x))
        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: " ".join([word for word in x.split() if word not in self.stopwords]))
        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: " ".join([self.lemmatizer.lemmatize(word) for word in x.split()]))
        self.features['clean_msg'] = self.features['clean_msg'].str.replace(r'\d+', '<num>', regex=True)
        self.features['clean_msg'] = self.features['clean_msg'].str.strip()
        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: " ".join([word for word in x.split() if len(word) > 2]))

        return self.features
    
    def ngrams(self):
        spam_blob = self.features[self.features["target"]==1]["clean_msg"].str.cat()
        ham_blob = self.features[self.features["target"]==0]["clean_msg"].str.cat()

        spam_tokens = self.tokenize_words(spam_blob)
        ham_tokens = self.tokenize_words(ham_blob)

        spam_bigrams = Counter(list(ngrams(spam_tokens,2)))
        spam_trigrams = Counter(list(ngrams(spam_tokens,3)))

        ham_bigrams = Counter(list(ngrams(ham_tokens,2)))
        ham_trigrams = Counter(list(ngrams(ham_tokens,3)))

        return spam_bigrams, spam_trigrams, ham_bigrams, ham_trigrams

    
    def feature_eng(self):
        self.features['char_count'] = self.df['Message'].apply(len)
        self.features['word_count'] = self.features['clean_msg'].apply(lambda msg: len(self.tokenize_words(msg)))
        self.features['digit_count'] = self.df['Message'].apply(lambda x: sum(c.isdigit() for c in x))
        self.features['question_count'] = self.df['Message'].apply(lambda x: x.count('?'))
        self.features['exclamation_count'] = self.df['Message'].apply(lambda x: x.count('!'))
        self.features['dollar_count'] = self.df['Message'].apply(lambda x: x.count('$') + x.count('€') + x.count('£'))
        self.features['cap_ratio'] = self.df['Message'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x))
        self.features['unique_words'] = self.features['clean_msg'].apply(lambda x: len(set(x.split())))
        self.features['repitition_factor'] = self.features['word_count'].astype(float) / self.features['unique_words'].astype(float)
        self.features['sentiment'] = self.features['clean_msg'].apply(lambda x: self.sia.polarity_scores(x)['compound'])

        self.features.to_csv("prepared_data.csv")

        return self.features
    
    def word_count(self, word_bank: list) -> dict:
        pattern = r"[^\s./!?]+"
        tokenizer = RegexpTokenizer(pattern)
        counts = list()
        for msg in word_bank:
            words_count = dict()
            words = tokenizer.tokenize(msg)
            for word in words:
                if words_count.keys().__contains__(word) == False:
                    words_count[word] = words.count(word)
                else:
                    continue
            counts.append(words_count)

        return counts
    
    def cap_count(self, tokens: list) -> int:
        count = int(0)
        for word in tokens:
            if word.isupper() == True:
                count += 1
            else:
                continue
        
        return count
    


f = Transformer(df)
f.clean_msg()


spam_bigrams, spam_trigrams, ham_bigrams, ham_trigrams = f.ngrams()


df_spam_bigrams = pd.DataFrame({"bigram": spam_bigrams.keys(), "count": spam_bigrams.values()})
df_spam_bigrams.to_csv("spam_bigrams.csv")
df_spam_trigrams = pd.DataFrame({"trigram": spam_trigrams.keys(), "count": spam_trigrams.values()})
df_spam_trigrams.to_csv("spam_trigrams.csv")
df_ham_bigrams = pd.DataFrame({"bigrams": ham_bigrams.keys(), "count": ham_bigrams.values()})
df_ham_bigrams.to_csv("ham_bigrams.csv")
df_ham_trigrams = pd.DataFrame({"trigrams": ham_trigrams.keys(), "count": ham_trigrams.values()})
df_ham_trigrams.to_csv("ham_trigrams.csv")


df_spam_bigrams.sort_values("count",ascending=False).head(20)["count"].sum()


df_spam_bigrams.shape


f.feature_eng()


f.features.describe()


plt.hist(x = f.features[f.features["target"]==0]["sentiment"].values, bins = 20, alpha = 0.85)
plt.vlines(label = "Median",linestyles='--',x = f.features[f.features["target"] == 0]["sentiment"].median(), ymin=0, ymax=2100, color = 'red')
plt.vlines(label = "Mean",linestyles='-.',x = f.features[f.features["target"] == 0]["sentiment"].mean(), ymin=0, ymax=2100, color = 'black')
plt.title("Distribution of Sentiment Analysis Scores for Ham Messages")
plt.xlabel("Vader Sentiment Score (Compounded)")
plt.ylabel("Frequency of Score")
plt.grid(True, alpha = 0.5, linestyle = '-.')
plt.ylim(0,2100)
plt.legend()
plt.savefig("Ham Sentiment distibution")
plt.show()


plt.hist(x = f.features[f.features["target"]==1]["sentiment"].values, bins = 20, alpha = 0.85)
plt.vlines(label = "Median",linestyles='--',x = f.features[f.features["target"] == 1]["sentiment"].median(), ymin=0, ymax=130, color = 'red')
plt.vlines(label = "Mean",linestyles='-.',x = f.features[f.features["target"] == 1]["sentiment"].mean(), ymin=0, ymax=130, color = 'black')
plt.title("Distribution of Sentiment Analysis Scores for Spam Messages")
plt.xlabel("Vader Sentiment Score (Compounded)")
plt.ylabel("Frequency of Score")
plt.ylim(0,130)
plt.grid(True, alpha = 0.5, linestyle = '-.')
plt.legend()
plt.savefig("Spam Sentiment distibution")
plt.show()


plt.hist(x = f.features[f.features["target"]==1]["cap_ratio"].values, bins = 20, alpha = 0.85)
plt.vlines(label = "Median",linestyles='--',x = f.features[f.features["target"] == 1]["cap_ratio"].median(), ymin=0, ymax=190, color = 'red')
plt.vlines(label = "Mean",linestyles='-.',x = f.features[f.features["target"] == 1]["cap_ratio"].mean(), ymin=0, ymax=190, color = 'black')
plt.title("Distribution of Capitalized Ratio for Spam Messages")
plt.xlabel("Cap Ratio (no. of capitalized characters / total characters)")
plt.ylabel("Frequency of messages")
plt.ylim(0,190)
plt.grid(True, alpha = 0.5, linestyle = '-.')
plt.legend()
plt.savefig("Spam Capitalized distibution")
plt.show()


plt.hist(x = f.features[f.features["target"]==0]["cap_ratio"].values, bins = 20, alpha = 0.85)
plt.vlines(label = "Median",linestyles='--',x = f.features[f.features["target"] == 0]["cap_ratio"].median(), ymin=0, ymax=4000, color = 'red')
plt.vlines(label = "Mean",linestyles='-.',x = f.features[f.features["target"] == 0]["cap_ratio"].mean(), ymin=0, ymax=4000, color = 'black')
plt.title("Distribution of Capitalized Ratio for Ham Messages")
plt.xlabel("Cap Ratio (no. of capitalized characters / total characters)")
plt.ylabel("Frequency of messages")
plt.ylim(0,4000)
plt.grid(True, alpha = 0.5, linestyle = '-.')
plt.legend()
plt.savefig("Ham Capitalized distibution")
plt.show()


df_spam_bigrams.sort_values(by="count",ascending=False).head(10)


sample = df_spam_bigrams.sort_values(by="count",ascending=False).head(15)
sample["bigram"] = sample["bigram"].apply(lambda x: x[0] + " " + x[1])
plt.barh(y = sample["bigram"].values, width = sample["count"],alpha=0.85)
plt.xlabel("Count of Bigrams")
plt.title("Most Commonly used Bigrams in Spam Messages")
plt.ylabel("Bigram")
plt.grid(True,linestyle='-.')
plt.savefig("Most Commonly used Bigrams in Spam Messages")
plt.show()


import seaborn as sns

a = f.features[f.features.describe().columns].corr()
sns.heatmap(data = f.features[f.features.describe().columns].corr())
plt.title("Heatmap of Pearson Correlation Matrix")


f.features["sentiment"].values


description = f.features.describe()


f.features[f.features["question_count"] > 1]["clean_msg"].index


df.iloc[f.features[f.features["question_count"] > 0]["clean_msg"].index,:]["Class"].value_counts().keys().to_list()


question_proba = []
for i in range(description["question_count"]["max"].astype(int)):
    values = df.iloc[f.features[f.features["question_count"] > i]["clean_msg"].index,:]["Class"].value_counts()
    try:
        hams = values["ham"]
    except KeyError:
        hams = 0
    
    try:
        spams = values["spam"]
    except KeyError:
        spams = 0
    question_proba.append((spams/(spams + hams))*100)

exclamation_proba = []
for i in range(description["exclamation_count"]["max"].astype(int)):
    values = df.iloc[f.features[f.features["exclamation_count"] > i]["clean_msg"].index,:]["Class"].value_counts()
    try:
        hams = values["ham"]
    except KeyError:
        hams = 0
    
    try:
        spams = values["spam"]
    except KeyError:
        spams = 0
    exclamation_proba.append((spams/(spams + hams))*100)

dollar_proba = []
for i in range(description["dollar_count"]["max"].astype(int)):
    values = df.iloc[f.features[f.features["dollar_count"] > i]["clean_msg"].index,:]["Class"].value_counts()
    try:
        hams = values["ham"]
    except KeyError:
        hams = 0
    
    try:
        spams = values["spam"]
    except KeyError:
        spams = 0
    dollar_proba.append((spams/(spams + hams))*100)


description["exclamation_count"]["max"].astype(int)


import matplotlib.pyplot as plt

questions = description["question_count"]["max"].astype(int)
exclamation = description["exclamation_count"]["max"].astype(int)
dollar = description["dollar_count"]["max"].astype(int)


plt.plot(range(questions),question_proba, marker='o', label='?')
plt.plot(range(exclamation),exclamation_proba, marker='o', label = "!")
plt.plot(range(dollar),dollar_proba, marker='o', label = "$,£,€")
plt.title("Probability of Spam given number of Special characters in SMS")
plt.ylabel("Probability of Spam (%)")
plt.xlabel("Count of Special character in SMS")
plt.grid(True)
plt.legend()
plt.savefig("exclamation.png")


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import classification_report,accuracy_score
from sklearn.neighbors import KNeighborsClassifier
import mlflow
import mlflow.sklearn


spam_corpus = f.features[f.features["target"]==1]["clean_msg"].to_list()
tf = TfidfVectorizer()
tf.fit(spam_corpus)
spam_vocab = pd.DataFrame()
words = list()
counts = list()
idf = list()
for k,v,i in zip(tf.vocabulary_.keys(), tf.vocabulary_.values(),tf.idf_):
    words.append(k)
    counts.append(v)
    idf.append(i)

spam_vocab["word"] = words
spam_vocab["count"] = counts
spam_vocab["idf"] = idf


spam_vocab.sort_values(by="idf", ascending=False)


f.features = f.features.fillna(0)

X_train, X_test, y_train, y_test = train_test_split(
    f.features[["char_count","digit_count","exclamation_count","dollar_count","sentiment"]],
    f.features["target"],
    test_size=0.2,
    random_state=42,
    stratify=f.features["target"]
)


X_train = kf.split(X_train, y_train)





pipe = Pipeline([
    ("tfidf_vectorizer", TfidfVectorizer()),
    ("SVM_classifier", SVC())
])

pipe.fit(X_train,y_train)
y_preds = pipe.predict(X_test)

print(classification_report(y_test, y_preds))


grid = {
    "n_neighbors": range(3,16,2),
    "weights": ["uniform", "distance"],
    "algorithm": ["auto","ball_tree","kd_tree","brute"]
    # "n_jobs": range(2,8,2)
}

knn = KNeighborsClassifier()

grid_search = GridSearchCV(
    estimator=knn,
    param_grid=grid,
    scoring=["recall","precision","f1"],
    verbose=2,
    refit=False,
    return_train_score=True
)

with mlflow.start_run():

    grid_search.fit(X_train, y_train)

    mlflow.log_param('Best Parameters', grid_search.best_params_)
    mlflow.log_metric('Best Score', grid_search.best_score_)

    results_df = pd.DataFrame(grid_search.cv_results_)
    mlflow.log_param('All Results', results_df.to_dict())

    y_pred = grid_search.best_estimator_.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric('Test Accuracy', test_accuracy)

    report = classification_report(y_test, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    report_csv = "classification_report.csv"
    report_df.to_csv(report_csv)
    mlflow.log_artifact(report_csv)

    pivot_table = results_df.pivot('param_n_estimators', 'param_max_depth', 'mean_test_score')
    plt.figure(figsize=(8, 6))
    sns.heatmap(pivot_table, annot=True, cmap="YlGnBu")
    plt.title("Grid Search Results - Accuracy")
    plt.savefig("grid_search_heatmap.png")
    mlflow.log_artifact("grid_search_heatmap.png")

    mlflow.sklearn.log_model(grid_search.best_estimator_, "model")


help(grid_search)


GS.best_score_


knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(X_train, y_train)
y_preds = knn.predict(X_test)

print(cross_val_score(knn, X_train, y_train, cv=5, scoring='precision'))

print(classification_report(y_test, y_preds))


len(a[a==False])


f.features.iloc[3625,1]
