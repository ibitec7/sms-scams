{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as pd\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.feature_extraction._tfidf_vectorizer import TfidfTransformer\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import cupy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ibrahim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ibrahim/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ibrahim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"~/nltk_data/corpora/stopwords.zip\") == False:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "if os.path.exists(\"~/nltk_data/sentiment/vader_lexicon.zip\") == False:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "\n",
    "if os.path.exists(\"~/nltk_data/corpora/wordnet.zip\") == False:\n",
    "    nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TfidfTransformer in module cuml.feature_extraction._tfidf:\n",
      "\n",
      "class TfidfTransformer(cuml.internals.base.Base)\n",
      " |  TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False, handle=None, verbose=False, output_type=None)\n",
      " |\n",
      " |  Transform a count matrix to a normalized tf or tf-idf representation\n",
      " |  Tf means term-frequency while tf-idf means term-frequency times inverse\n",
      " |  document-frequency. This is a common term weighting scheme in information\n",
      " |  retrieval, that has also found good use in document classification.\n",
      " |  The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
      " |  token in a given document is to scale down the impact of tokens that occur\n",
      " |  very frequently in a given corpus and that are hence empirically less\n",
      " |  informative than features that occur in a small fraction of the training\n",
      " |  corpus.\n",
      " |  The formula that is used to compute the tf-idf for a term t of a document d\n",
      " |  in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
      " |  computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
      " |  n is the total number of documents in the document set and df(t) is the\n",
      " |  document frequency of t; the document frequency is the number of documents\n",
      " |  in the document set that contain the term t. The effect of adding \"1\" to\n",
      " |  the idf in the equation above is that terms with zero idf, i.e., terms\n",
      " |  that occur in all documents in a training set, will not be entirely\n",
      " |  ignored.\n",
      " |  (Note that the idf formula above differs from the standard textbook\n",
      " |  notation that defines the idf as\n",
      " |  idf(t) = log [ n / (df(t) + 1) ]).\n",
      " |  If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
      " |  numerator and denominator of the idf as if an extra document was seen\n",
      " |  containing every term in the collection exactly once, which prevents\n",
      " |  zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n",
      " |  Furthermore, the formulas used to compute tf and idf depend\n",
      " |  on parameter settings that correspond to the SMART notation used in IR\n",
      " |  as follows:\n",
      " |  Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n",
      " |  ``sublinear_tf=True``.\n",
      " |  Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
      " |  Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n",
      " |  when ``norm=None``.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |\n",
      " |  norm : {'l1', 'l2'}, default='l2'\n",
      " |      Each output row will have unit norm, either:\n",
      " |       * 'l2': Sum of squares of vector elements is 1. The cosine similarity\n",
      " |         between two vectors is their dot product when l2 norm has been\n",
      " |         applied.\n",
      " |       * 'l1': Sum of absolute values of vector elements is 1.\n",
      " |  use_idf : bool, default=True\n",
      " |      Enable inverse-document-frequency reweighting.\n",
      " |  smooth_idf : bool, default=True\n",
      " |      Smooth idf weights by adding one to document frequencies, as if an\n",
      " |      extra document was seen containing every term in the collection\n",
      " |      exactly once. Prevents zero divisions.\n",
      " |  sublinear_tf : bool, default=False\n",
      " |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      " |  handle : cuml.Handle\n",
      " |      Specifies the cuml.handle that holds internal CUDA state for\n",
      " |      computations in this model. Most importantly, this specifies the CUDA\n",
      " |      stream that will be used for the model's computations, so users can\n",
      " |      run different models concurrently in different streams by creating\n",
      " |      handles in several streams.\n",
      " |      If it is None, a new one is created.\n",
      " |  verbose : int or boolean, default=False\n",
      " |      Sets logging level. It must be one of `cuml.common.logger.level_*`.\n",
      " |      See :ref:`verbosity-levels` for more info.\n",
      " |  output_type : {'input', 'array', 'dataframe', 'series', 'df_obj',         'numba', 'cupy', 'numpy', 'cudf', 'pandas'}, default=None\n",
      " |      Return results and set estimator attributes to the indicated output\n",
      " |      type. If None, the output type set at the module level\n",
      " |      (`cuml.global_settings.output_type`) will be used. See\n",
      " |      :ref:`output-data-type-configuration` for more info.\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  idf_ : array of shape (n_features)\n",
      " |      The inverse document frequency (IDF) vector; only defined\n",
      " |      if ``use_idf`` is True.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      TfidfTransformer\n",
      " |      cuml.internals.base.Base\n",
      " |      cuml.internals.mixins.TagsMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__ = inner_f(self, *, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False, handle=None, verbose=False, output_type=None) from cuml.internals.api_decorators._deprecate_pos_args.__call__.<locals>\n",
      " |\n",
      " |  fit(self, X) -> 'TfidfTransformer'\n",
      " |      Learn the idf vector (global term weights).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape n_samples, n_features\n",
      " |          A matrix of term/token counts.\n",
      " |\n",
      " |  fit_transform(self, X, copy=True)\n",
      " |      Fit TfidfTransformer to X, then transform X.\n",
      " |      Equivalent to fit(X).transform(X).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of (n_samples, n_features)\n",
      " |          A matrix of term/token counts\n",
      " |      copy : bool, default=True\n",
      " |          Whether to copy X and operate on the copy or perform in-place\n",
      " |          operations.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      vectors : array-like of shape (n_samples, n_features)\n",
      " |\n",
      " |  transform(self, X, copy=True)\n",
      " |      Transform a count matrix to a tf or tf-idf representation\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of (n_samples, n_features)\n",
      " |          A matrix of term/token counts\n",
      " |      copy : bool, default=True\n",
      " |          Whether to copy X and operate on the copy or perform in-place\n",
      " |          operations.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      vectors : array-like of shape (n_samples, n_features)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  idf_\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from cuml.internals.base.Base:\n",
      " |\n",
      " |  __getattr__(self, attr)\n",
      " |      Redirects to `solver_model` if the attribute exists.\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Pretty prints the arguments of a class using Scikit-learn standard :)\n",
      " |\n",
      " |  __setstate__(self, d)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Returns a dict of all params owned by this class. If the child class\n",
      " |      has appropriately overridden the `_get_param_names` method and does not\n",
      " |      need anything other than what is there in this method, then it doesn't\n",
      " |      have to override this method\n",
      " |\n",
      " |  set_nvtx_annotations(self)\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Accepts a dict of params and updates the corresponding ones owned by\n",
      " |      this class. If the child class has appropriately overridden the\n",
      " |      `_get_param_names` method and does not need anything other than what is,\n",
      " |      there in this method, then it doesn't have to override this method\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from cuml.internals.mixins.TagsMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Spam_SMS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    def __doc__(self):\n",
    "        \"\"\"\n",
    "            Transformer Class to clean and prepare SMS Ham data:\n",
    "\n",
    "            Attributes:\n",
    "            -----------\n",
    "                self.spam_bank (list):\n",
    "\n",
    "                self.ham_bank  (list):\n",
    "\n",
    "                self.spam_urls (list):\n",
    "\n",
    "                self.ham_urls  (list):\n",
    "\n",
    "                self.features  (pd.DataFrame):\n",
    "\n",
    "                self.df  (pd.DataFrame):\n",
    "\n",
    "                self.stopwords (list):\n",
    "\n",
    "                self.lemmatizer (nltk.stem.WordNetLemmatizer):\n",
    "\n",
    "                self.sia (nltk.sentiment.vader.SentimentIntensityAnalyzer):\n",
    "\n",
    "                self.contractions (dict):\n",
    "\n",
    "            Methods:\n",
    "            --------\n",
    "                self.tokenize_words(self, message: str) -> list:\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.spam_bank = [l for l in df[df[\"Class\"] == \"spam\"]['Message']]\n",
    "        # self.spam = ' '.join(df[df['Class'] == 'spam'][\"Message\"])\n",
    "        self.ham_bank = [l for l in df[df[\"Class\"] == \"ham\"][\"Message\"]]\n",
    "        self.spam_urls = [url for msg in self.spam_bank for url in re.findall(r'http[s]:\\/\\/[\\S]+', msg)]\n",
    "        self.ham_urls = [url for msg in self.ham_bank for url in re.findall(r'http[s]:\\/\\/[\\S]+', msg)]\n",
    "        self.features = pd.DataFrame({}).to_pandas()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.df = df.copy()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.contractions = {\n",
    "                                \"can't\": \"cannot\",\n",
    "                                \"won't\": \"will not\",\n",
    "                                \"n't\": \" not\",\n",
    "                                \"'re\": \" are\",\n",
    "                                \"'s\": \" is\",\n",
    "                                \"'d\": \" would\",\n",
    "                                \"'ll\": \" will\",\n",
    "                                \"'ve\": \" have\",\n",
    "                                \"'m\": \" am\",\n",
    "                            }\n",
    "\n",
    "    def tokenize_words(self, message: str) -> list:\n",
    "        tokenizer = RegexpTokenizer(r\"[^\\s.,?!]+\")\n",
    "        tokens = tokenizer.tokenize(message)\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def expand_contractions(self, text, contractions_dict):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())), \n",
    "                                        flags=re.IGNORECASE | re.DOTALL)\n",
    "        def replace(match):\n",
    "            return contractions_dict[match.group(0).lower()]\n",
    "        \n",
    "        return contractions_pattern.sub(replace, text)\n",
    "    \n",
    "    def clean_msg(self):\n",
    "        self.features['clean_msg'] = self.df['Message'].str.lower()\n",
    "        self.features[\"target\"] = self.df[\"Class\"].apply(lambda x: 1 if x == \"spam\" else 0)\n",
    "        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: self.expand_contractions(x, self.contractions))\n",
    "        self.features['clean_msg'] = self.features['clean_msg'].str.replace(r'http[s]:\\/\\/[\\S]+', '<url>', regex=True)\n",
    "        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: x.translate(str.maketrans('','', string.punctuation)))\n",
    "        \n",
    "        sym_spell = SymSpell(max_dictionary_edit_distance=4, prefix_length=7)\n",
    "        dictionary = \"frequency_dictionary_en_82_765.txt\"\n",
    "        sym_spell.load_dictionary(dictionary,term_index=0,count_index=1)\n",
    "\n",
    "        def correct(msg: str) -> str:\n",
    "            suggestions = []\n",
    "            for word in msg.split():\n",
    "                suggestion = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "                if suggestion:\n",
    "                    suggestions.append(suggestion[0].term)\n",
    "                else:\n",
    "                    suggestions.append(word)\n",
    "\n",
    "            return \" \".join(suggestions)\n",
    "        \n",
    "        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: correct(x))\n",
    "        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: \" \".join([word for word in x.split() if word not in self.stopwords]))\n",
    "        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: \" \".join([self.lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "        self.features['clean_msg'] = self.features['clean_msg'].str.replace(r'\\d+', '<num>', regex=True)\n",
    "        self.features['clean_msg'] = self.features['clean_msg'].str.strip()\n",
    "        self.features['clean_msg'] = self.features['clean_msg'].apply(lambda x: \" \".join([word for word in x.split() if len(word) > 2]))\n",
    "\n",
    "        return self.features\n",
    "    \n",
    "    def ngrams(self):\n",
    "        spam_blob = self.features[self.features[\"target\"]==1][\"clean_msg\"].str.cat()\n",
    "        ham_blob = self.features[self.features[\"target\"]==0][\"clean_msg\"].str.cat()\n",
    "\n",
    "        spam_tokens = self.tokenize_words(spam_blob)\n",
    "        ham_tokens = self.tokenize_words(ham_blob)\n",
    "\n",
    "        spam_bigrams = Counter(list(ngrams(spam_tokens,2)))\n",
    "        spam_trigrams = Counter(list(ngrams(spam_tokens,3)))\n",
    "\n",
    "        ham_bigrams = Counter(list(ngrams(ham_tokens,2)))\n",
    "        ham_trigrams = Counter(list(ngrams(ham_tokens,3)))\n",
    "\n",
    "        return spam_bigrams, spam_trigrams, ham_bigrams, ham_trigrams\n",
    "\n",
    "    \n",
    "    def feature_eng(self):\n",
    "        self.features['char_count'] = self.df['Message'].apply(len)\n",
    "        self.features['word_count'] = self.features['clean_msg'].apply(lambda msg: len(self.tokenize_words(msg)))\n",
    "        self.features['digit_count'] = self.df['Message'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "        self.features['question_count'] = self.df['Message'].apply(lambda x: x.count('?'))\n",
    "        self.features['exclamation_count'] = self.df['Message'].apply(lambda x: x.count('!'))\n",
    "        self.features['dollar_count'] = self.df['Message'].apply(lambda x: x.count('$') + x.count('€') + x.count('£'))\n",
    "        self.features['cap_ratio'] = self.df['Message'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x))\n",
    "        self.features['unique_words'] = self.features['clean_msg'].apply(lambda x: len(set(x.split())))\n",
    "        self.features['repitition_factor'] = self.features['word_count'].astype(float) / self.features['unique_words'].astype(float)\n",
    "        self.features['sentiment'] = self.features['clean_msg'].apply(lambda x: self.sia.polarity_scores(x)['compound'])\n",
    "\n",
    "        self.features.to_csv(\"prepared_data.csv\")\n",
    "\n",
    "        return self.features\n",
    "    \n",
    "    def word_count(self, word_bank: list) -> dict:\n",
    "        pattern = r\"[^\\s./!?]+\"\n",
    "        tokenizer = RegexpTokenizer(pattern)\n",
    "        counts = list()\n",
    "        for msg in word_bank:\n",
    "            words_count = dict()\n",
    "            words = tokenizer.tokenize(msg)\n",
    "            for word in words:\n",
    "                if words_count.keys().__contains__(word) == False:\n",
    "                    words_count[word] = words.count(word)\n",
    "                else:\n",
    "                    continue\n",
    "            counts.append(words_count)\n",
    "\n",
    "        return counts\n",
    "    \n",
    "    def cap_count(self, tokens: list) -> int:\n",
    "        count = int(0)\n",
    "        for word in tokens:\n",
    "            if word.isupper() == True:\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>point crazy available bug great world buffet c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lar joking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry wkly comp win cup final tit list ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dun say early hor already say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah think life around though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>time tried contact £&lt;num&gt; pound prize claim ea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>going esplanade home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>pity mood sony suggestion</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>guy bitching acted like would interested buyin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>roll true name</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_msg  target\n",
       "0     point crazy available bug great world buffet c...       0\n",
       "1                                            lar joking       0\n",
       "2     free entry wkly comp win cup final tit list ma...       1\n",
       "3                         dun say early hor already say       0\n",
       "4                          nah think life around though       0\n",
       "...                                                 ...     ...\n",
       "5569  time tried contact £<num> pound prize claim ea...       1\n",
       "5570                               going esplanade home       0\n",
       "5571                          pity mood sony suggestion       0\n",
       "5572  guy bitching acted like would interested buyin...       0\n",
       "5573                                     roll true name       0\n",
       "\n",
       "[5574 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Transformer(df.to_pandas())\n",
    "f.clean_msg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_bigrams, spam_trigrams, ham_bigrams, ham_trigrams = f.ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spam_bigrams.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /home/ibrahim/anaconda3/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_spam_bigrams \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigram\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(spam_bigrams\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m: spam_bigrams\u001b[38;5;241m.\u001b[39mvalues()})\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_spam_bigrams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspam_bigrams.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_spam_trigrams \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigram\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(spam_trigrams\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m: spam_trigrams\u001b[38;5;241m.\u001b[39mvalues()})\n\u001b[1;32m      4\u001b[0m df_spam_trigrams\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspam_trigrams.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids/lib/python3.12/site-packages/cudf/core/dataframe.py:6946\u001b[0m, in \u001b[0;36mDataFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, columns, header, index, encoding, compression, lineterminator, chunksize, storage_options)\u001b[0m\n\u001b[1;32m   6944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lineterminator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6945\u001b[0m     lineterminator \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlinesep\n\u001b[0;32m-> 6946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6947\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6949\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6959\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids/lib/python3.12/site-packages/cudf/utils/performance_tracking.py:51\u001b[0m, in \u001b[0;36m_performance_tracking.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nvtx\u001b[38;5;241m.\u001b[39menabled():\n\u001b[1;32m     44\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m     45\u001b[0m         nvtx\u001b[38;5;241m.\u001b[39mannotate(\n\u001b[1;32m     46\u001b[0m             message\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids/lib/python3.12/site-packages/cudf/io/csv.py:232\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(df, path_or_buf, sep, na_rep, columns, header, index, encoding, compression, lineterminator, chunksize, storage_options)\u001b[0m\n\u001b[1;32m    221\u001b[0m         libcudf\u001b[38;5;241m.\u001b[39mcsv\u001b[38;5;241m.\u001b[39mwrite_csv(\n\u001b[1;32m    222\u001b[0m             df,\n\u001b[1;32m    223\u001b[0m             path_or_buf\u001b[38;5;241m=\u001b[39mfile_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m             index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[43mlibcudf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrows_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows_per_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_as_string:\n\u001b[1;32m    244\u001b[0m     path_or_buf\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mcsv.pyx:326\u001b[0m, in \u001b[0;36mcudf._lib.csv.write_csv\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcsv.pyx:457\u001b[0m, in \u001b[0;36mpylibcudf.io.csv.write_csv\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcsv.pyx:475\u001b[0m, in \u001b[0;36mpylibcudf.io.csv.write_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /home/ibrahim/anaconda3/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "df_spam_bigrams = pd.DataFrame({\"bigram\": str(spam_bigrams.keys()), \"count\": spam_bigrams.values()})\n",
    "df_spam_bigrams.to_csv(\"spam_bigrams.csv\")\n",
    "df_spam_trigrams = pd.DataFrame({\"trigram\": str(spam_trigrams.keys()), \"count\": spam_trigrams.values()})\n",
    "df_spam_trigrams.to_csv(\"spam_trigrams.csv\")\n",
    "df_ham_bigrams = pd.DataFrame({\"bigrams\": str(ham_bigrams.keys()), \"count\": ham_bigrams.values()})\n",
    "df_ham_bigrams.to_csv(\"ham_bigrams.csv\")\n",
    "df_ham_trigrams = pd.DataFrame({\"trigrams\": str(ham_trigrams.keys()), \"count\": ham_trigrams.values()})\n",
    "df_ham_trigrams.to_csv(\"ham_trigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[free, entry]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[entry, wkly]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[wkly, comp]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[comp, win]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[win, cup]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>[name, house]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>[house, postcodetime]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>[postcodetime, tried]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>[£&lt;num&gt;, pound]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>[minute, btnationalrate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5430 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        bigram  count\n",
       "0                [free, entry]      8\n",
       "1                [entry, wkly]      5\n",
       "2                 [wkly, comp]      4\n",
       "3                  [comp, win]      2\n",
       "4                   [win, cup]      2\n",
       "...                        ...    ...\n",
       "5425             [name, house]      1\n",
       "5426     [house, postcodetime]      1\n",
       "5427     [postcodetime, tried]      1\n",
       "5428           [£<num>, pound]      1\n",
       "5429  [minute, btnationalrate]      1\n",
       "\n",
       "[5430 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>target</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>digit_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>dollar_count</th>\n",
       "      <th>cap_ratio</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>repitition_factor</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>point crazy available bug great world buffet c...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lar joking</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry wkly comp win cup final tit list ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>17</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>0.7964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dun say early hor already say</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>5</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah think life around though</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>time tried contact £&lt;num&gt; pound prize claim ea...</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056250</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>going esplanade home</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>pity mood sony suggestion</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>guy bitching acted like would interested buyin...</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>roll true name</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_msg  target  char_count  \\\n",
       "0     point crazy available bug great world buffet c...       0         111   \n",
       "1                                            lar joking       0          29   \n",
       "2     free entry wkly comp win cup final tit list ma...       1         155   \n",
       "3                         dun say early hor already say       0          49   \n",
       "4                          nah think life around though       0          61   \n",
       "...                                                 ...     ...         ...   \n",
       "5569  time tried contact £<num> pound prize claim ea...       1         160   \n",
       "5570                               going esplanade home       0          36   \n",
       "5571                          pity mood sony suggestion       0          57   \n",
       "5572  guy bitching acted like would interested buyin...       0         125   \n",
       "5573                                     roll true name       0          26   \n",
       "\n",
       "      word_count  digit_count  question_count  exclamation_count  \\\n",
       "0             10            0               0                  0   \n",
       "1              2            0               0                  0   \n",
       "2             20           25               0                  0   \n",
       "3              6            0               0                  0   \n",
       "4              5            0               0                  0   \n",
       "...          ...          ...             ...                ...   \n",
       "5569          13           21               0                  1   \n",
       "5570           3            0               1                  0   \n",
       "5571           4            0               1                  0   \n",
       "5572          13            0               0                  0   \n",
       "5573           3            0               0                  0   \n",
       "\n",
       "      dollar_count  cap_ratio  unique_words  repitition_factor  sentiment  \n",
       "0                0   0.027027            10           1.000000     0.4019  \n",
       "1                0   0.068966             2           1.000000     0.2263  \n",
       "2                0   0.064516            17           1.176471     0.7964  \n",
       "3                0   0.040816             5           1.200000     0.0000  \n",
       "4                0   0.032787             5           1.000000    -0.1027  \n",
       "...            ...        ...           ...                ...        ...  \n",
       "5569             1   0.056250            13           1.000000     0.7351  \n",
       "5570             0   0.027778             3           1.000000     0.0000  \n",
       "5571             0   0.035088             4           1.000000    -0.2960  \n",
       "5572             0   0.016000            13           1.000000     0.7506  \n",
       "5573             0   0.076923             3           1.000000     0.4215  \n",
       "\n",
       "[5574 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.feature_eng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_spam_bigrams.sort_values(by=\"count\",ascending=False).head(15)\n",
    "sample[\"bigram\"] = sample[\"bigram\"].apply(lambda x: x[0] + \" \" + x[1])\n",
    "plt.barh(y = sample[\"bigram\"].values, width = sample[\"count\"],alpha=0.85)\n",
    "plt.xlabel(\"Count of Bigrams\")\n",
    "plt.title(\"Most Commonly used Bigrams in Spam Messages\")\n",
    "plt.ylabel(\"Bigram\")\n",
    "plt.grid(True,linestyle='-.')\n",
    "plt.savefig(\"Most Commonly used Bigrams in Spam Messages\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.features[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = f.features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.features[f.features[\"question_count\"] > 1][\"clean_msg\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import N\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import mlflow\n",
    "import seaborn as sns\n",
    "from mlflow import create_experiment\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.create_experiment(\"spams\")\n",
    "mlflow.set_experiment(\"spams\")\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_corpus = f.features[f.features[\"target\"]==1][\"clean_msg\"].to_list()\n",
    "tf = TfidfVectorizer()\n",
    "tf.fit(spam_corpus)\n",
    "spam_vocab = pd.DataFrame()\n",
    "words = list()\n",
    "counts = list()\n",
    "idf = list()\n",
    "for k,v,i in zip(tf.vocabulary_.keys(), tf.vocabulary_.values(),tf.idf_):\n",
    "    words.append(k)\n",
    "    counts.append(v)\n",
    "    idf.append(i)\n",
    "\n",
    "spam_vocab[\"word\"] = words\n",
    "spam_vocab[\"count\"] = counts\n",
    "spam_vocab[\"idf\"] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_vocab.sort_values(by=\"idf\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            f.features[\"clean_msg\"],\n",
    "            f.features[\"target\"],\n",
    "            random_state=42,\n",
    "            stratify=f.features[\"target\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>target</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>digit_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>dollar_count</th>\n",
       "      <th>cap_ratio</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>repitition_factor</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>point crazy available bug great world buffet c...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lar joking</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry wkly comp win cup final tit list ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>17</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>0.7964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dun say early hor already say</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>5</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah think life around though</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>time tried contact £&lt;num&gt; pound prize claim ea...</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056250</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>going esplanade home</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>pity mood sony suggestion</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>guy bitching acted like would interested buyin...</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>roll true name</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_msg  target  char_count  \\\n",
       "0     point crazy available bug great world buffet c...       0         111   \n",
       "1                                            lar joking       0          29   \n",
       "2     free entry wkly comp win cup final tit list ma...       1         155   \n",
       "3                         dun say early hor already say       0          49   \n",
       "4                          nah think life around though       0          61   \n",
       "...                                                 ...     ...         ...   \n",
       "5569  time tried contact £<num> pound prize claim ea...       1         160   \n",
       "5570                               going esplanade home       0          36   \n",
       "5571                          pity mood sony suggestion       0          57   \n",
       "5572  guy bitching acted like would interested buyin...       0         125   \n",
       "5573                                     roll true name       0          26   \n",
       "\n",
       "      word_count  digit_count  question_count  exclamation_count  \\\n",
       "0             10            0               0                  0   \n",
       "1              2            0               0                  0   \n",
       "2             20           25               0                  0   \n",
       "3              6            0               0                  0   \n",
       "4              5            0               0                  0   \n",
       "...          ...          ...             ...                ...   \n",
       "5569          13           21               0                  1   \n",
       "5570           3            0               1                  0   \n",
       "5571           4            0               1                  0   \n",
       "5572          13            0               0                  0   \n",
       "5573           3            0               0                  0   \n",
       "\n",
       "      dollar_count  cap_ratio  unique_words  repitition_factor  sentiment  \n",
       "0                0   0.027027            10           1.000000     0.4019  \n",
       "1                0   0.068966             2           1.000000     0.2263  \n",
       "2                0   0.064516            17           1.176471     0.7964  \n",
       "3                0   0.040816             5           1.200000     0.0000  \n",
       "4                0   0.032787             5           1.000000    -0.1027  \n",
       "...            ...        ...           ...                ...        ...  \n",
       "5569             1   0.056250            13           1.000000     0.7351  \n",
       "5570             0   0.027778             3           1.000000     0.0000  \n",
       "5571             0   0.035088             4           1.000000    -0.2960  \n",
       "5572             0   0.016000            13           1.000000     0.7506  \n",
       "5573             0   0.076923             3           1.000000     0.4215  \n",
       "\n",
       "[5574 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m([\n\u001b[1;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m, TfidfVectorizer()),\n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m\"\u001b[39m, SVC())\n\u001b[1;32m      4\u001b[0m ])\n\u001b[1;32m      6\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"svc\", SVC())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4925    0\n",
       "2349    0\n",
       "1396    0\n",
       "475     0\n",
       "1473    0\n",
       "       ..\n",
       "3838    0\n",
       "423     0\n",
       "3824    0\n",
       "1836    0\n",
       "61      0\n",
       "Name: target, Length: 4180, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVClassifier():\n",
    "    def __init__(self):\n",
    "        f.features = f.features.fillna(0)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            f.features[\"clean_msg\"],\n",
    "            f.features[\"target\"],\n",
    "            random_state=42,\n",
    "            stratify=f.features[\"target\"]\n",
    "        )\n",
    "\n",
    "        self.pipe = Pipeline([\n",
    "            (\"tfidf\", TfidfVectorizer()),\n",
    "            (\"svc\", SVC())\n",
    "        ])\n",
    "\n",
    "        self.grid = {\n",
    "            \"tfidf__max_df\": [0.8,0.9],\n",
    "            \"tfidf__ngram_range\": [(1,1), (1,2), (2,2), (2,3)],\n",
    "            \"svc__C\": [0.1,1,10],\n",
    "            \"svc__kernel\": [\"linear\",\"rbf\"],\n",
    "            \"svc__gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "\n",
    "        self.grid_search = GridSearchCV(\n",
    "            estimator=self.pipe,\n",
    "            param_grid=self.grid,\n",
    "            scoring=[\"recall\",\"precision\",\"f1\"],\n",
    "            verbose=3,\n",
    "            refit=\"recall\",\n",
    "            return_train_score=True,\n",
    "            cv=4\n",
    "        )\n",
    "\n",
    "    def tuning(self):\n",
    "        self.grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def train(self):\n",
    "        with mlflow.start_run(run_name=\"svc_model_tuning\"):\n",
    "            mlflow.set_tag(\"model_name\",\"svc\")\n",
    "\n",
    "            self.tuning()\n",
    "\n",
    "            mlflow.log_param(self.grid_search.best_params_)\n",
    "            mlflow.log_metric(\"Best Recall Score\", self.grid_search.best_score_)\n",
    "\n",
    "            results_df = pd.DataFrame(self.grid_search.cv_results_)\n",
    "            mlflow.log_params(results_df.to_dict())\n",
    "\n",
    "            for i in range(0, len(results_df[\"params\"])):\n",
    "                mlflow.log_metrics({\"svc_mean_test_recall\": results_df.at[i,\"mean_test_recall\"],\n",
    "                                    \"svc_mean_test_precision\": results_df.at[i,\"mean_test_precision\"],\n",
    "                                    \"svc_mean_test_f1\": results_df.at[i,\"mean_test_f1\"],\n",
    "                                    \"svc_mean_train_recall\": results_df.at[i,\"mean_train_recall\"],\n",
    "                                    \"svc_mean_train_precision\": results_df.at[i,\"mean_train_precision\"],\n",
    "                                    \"svc_mean_train_f1\": results_df.at[i,\"mean_train_f1\"]}, step=i)\n",
    "                \n",
    "            y_pred = self.grid_search.best_estimator_.predict(X_test)\n",
    "                \n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            report = pd.DataFrame(report).transpose().to_csv(\"svc_classification_report.csv\")\n",
    "            mlflow.log_artifact(\"svc_classification_report.csv\")\n",
    "\n",
    "            pivot_table = results_df[['params','split0_test_recall', 'split1_test_recall',\n",
    "                'split2_test_recall', 'split3_test_recall',\n",
    "                'split4_test_recall', 'mean_test_recall']]\n",
    "            pivot_table[\"params\"] = pivot_table.index\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(data=pivot_table, annot=True, cmap=\"YlGnBu\")\n",
    "            plt.title(\"SVC Grid Search Results - Recall\")\n",
    "            plt.savefig(\"grid_svc_heatmap.png\")\n",
    "            mlflow.log_artifact(local_path=\"grid_svc_heatmap.png\", artifact_path=\"grid_svc_heatmap.png\")\n",
    "\n",
    "            mlflow.sklearn.log_model(self.grid_search.best_estimator_, artifact_path=\"best_svc_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 96 candidates, totalling 384 fits\n",
      "[CV 1/4] END svc__C=0.1, svc__gamma=scale, svc__kernel=linear, tfidf__max_df=0.8, tfidf__ngram_range=(1, 1); f1: (train=0.879, test=0.841) precision: (train=0.982, test=0.981) recall: (train=0.795, test=0.736) total time=   0.5s\n",
      "[CV 2/4] END svc__C=0.1, svc__gamma=scale, svc__kernel=linear, tfidf__max_df=0.8, tfidf__ngram_range=(1, 1); f1: (train=0.877, test=0.884) precision: (train=0.985, test=1.000) recall: (train=0.790, test=0.793) total time=   0.5s\n",
      "[CV 3/4] END svc__C=0.1, svc__gamma=scale, svc__kernel=linear, tfidf__max_df=0.8, tfidf__ngram_range=(1, 1); f1: (train=0.874, test=0.865) precision: (train=0.988, test=0.973) recall: (train=0.783, test=0.779) total time=   0.4s\n",
      "[CV 4/4] END svc__C=0.1, svc__gamma=scale, svc__kernel=linear, tfidf__max_df=0.8, tfidf__ngram_range=(1, 1); f1: (train=0.872, test=0.879) precision: (train=0.994, test=0.935) recall: (train=0.776, test=0.829) total time=   0.5s\n",
      "[CV 1/4] END svc__C=0.1, svc__gamma=scale, svc__kernel=linear, tfidf__max_df=0.8, tfidf__ngram_range=(1, 2); f1: (train=0.824, test=0.817) precision: (train=0.997, test=0.980) recall: (train=0.702, test=0.700) total time=   1.0s\n",
      "[CV 2/4] END svc__C=0.1, svc__gamma=scale, svc__kernel=linear, tfidf__max_df=0.8, tfidf__ngram_range=(1, 2); f1: (train=0.826, test=0.857) precision: (train=0.997, test=1.000) recall: (train=0.705, test=0.750) total time=   1.0s\n",
      "[CV 3/4] END svc__C=0.1, svc__gamma=scale, svc__kernel=linear, tfidf__max_df=0.8, tfidf__ngram_range=(1, 2); f1: (train=0.812, test=0.800) precision: (train=0.997, test=0.960) recall: (train=0.686, test=0.686) total time=   1.0s\n",
      "[CV 4/4] END svc__C=0.1, svc__gamma=scale, svc__kernel=linear, tfidf__max_df=0.8, tfidf__ngram_range=(1, 2); f1: (train=0.827, test=0.835) precision: (train=1.000, test=0.930) recall: (train=0.705, test=0.757) total time=   1.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m svc \u001b[38;5;241m=\u001b[39m SVClassifier()\n\u001b[0;32m----> 2\u001b[0m \u001b[43msvc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mSVClassifier.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvc_model_tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     39\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mset_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     44\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Recall Score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_search\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m, in \u001b[0;36mSVClassifier.tuning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtuning\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1314\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/science/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1272\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     j_indices\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   1271\u001b[0m     values\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m-> 1272\u001b[0m     indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(j_indices))\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fixed_vocab:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;66;03m# disable defaultdict behaviour\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svc = SVClassifier()\n",
    "svc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    # \"n_neighbors\": range(3,16,2),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"auto\",\"ball_tree\",\"kd_tree\",\"brute\"],\n",
    "    \"n_jobs\": range(2,8,2)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_grid=grid,\n",
    "    scoring=[\"recall\",\"precision\",\"f1\"],\n",
    "    verbose=2,\n",
    "    refit=\"recall\",\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"knn_model_tuning\") as run:\n",
    "    mlflow.set_tag(\"model_name\", \"knn\")\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    mlflow.log_param('KNN Best Parameters', grid_search.best_params_)\n",
    "    mlflow.log_metric('KNN Best Score', grid_search.best_score_)\n",
    "\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    mlflow.log_param('All Results KNN', results_df.to_dict())\n",
    "\n",
    "    for i in range(0, len(results_df[\"params\"])):\n",
    "        mlflow.log_metrics({\"knn_mean_test_recall\": results_df.at[i,\"mean_test_recall\"],\n",
    "                            \"knn_mean_test_precision\": results_df.at[i,\"mean_test_precision\"],\n",
    "                            \"knn_mean_test_f1\": results_df.at[i,\"mean_test_f1\"],\n",
    "                            \"knn_mean_train_recall\": results_df.at[i,\"mean_train_recall\"],\n",
    "                            \"knn_mean_train_precision\": results_df.at[i,\"mean_train_precision\"],\n",
    "                            \"knn_mean_train_f1\": results_df.at[i,\"mean_train_f1\"]}, step=i)\n",
    "\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric('KNN Test Accuracy', test_accuracy)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_csv = \"classification_report.csv\"\n",
    "    report_df.to_csv(report_csv)\n",
    "    mlflow.log_artifact(report_csv)\n",
    "\n",
    "    pivot_table = results_df[['params','split0_test_recall', 'split1_test_recall',\n",
    "       'split2_test_recall', 'split3_test_recall',\n",
    "       'split4_test_recall', 'mean_test_recall']]\n",
    "    pivot_table[\"params\"] = pivot_table.index\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(data=pivot_table, annot=True, cmap=\"YlGnBu\")\n",
    "    plt.title(\"KNN Grid Search Results - Recall\")\n",
    "    plt.savefig(\"KNN_grid_search_heatmap.png\")\n",
    "    mlflow.log_artifact(local_path=\"KNN_grid_search_heatmap.png\", artifact_path=\"KNN_grid_search_heatmap.png\")\n",
    "\n",
    "    mlflow.sklearn.log_model(grid_search.best_estimator_, artifact_path=\"best_knn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table[\"params\"] = pivot_table[\"params\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table.params.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_preds = knn.predict(X_test)\n",
    "\n",
    "print(cross_val_score(knn, X_train, y_train, cv=5, scoring='precision'))\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a[a==False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.features.iloc[3625,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
